# Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models

<!-- <p align="center">
   <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRR3Wl7wsCgHpwUw1_eUXW_fptAPLL3FkhnW_rua0O1Ji_GIVrpTjY5LaKAhwO-WeARjnY_KNw0SYNJ/pubhtml" target="_blank">ğŸŒ Leaderboard (new)</a> | <a href="https://twitter.com/thukeg" target="_blank">ğŸ¦ Twitter</a> | <a href="mailto:agentbench@googlegroups.com">âœ‰ï¸ Google Group</a> | <a href="https://arxiv.org/abs/2308.03688" target="_blank">ğŸ“ƒ Paper </a>
</p> -->

<p align="center">
   <a href="https://arxiv.org/abs/2509.19191" target="_blank">ğŸ“ƒ Paper </a>
</p>


<!-- <p align="center">
ğŸ‘‹ Join our <a href="https://join.slack.com/t/agentbenchcol-huw1944/shared_invite/zt-20ixabcuv-31cFLBAkqGQxQkJqrWVEVg" target="_blank">Slack</a>  for <i>Q & A</i> or <i><b>collaboration</b> on next version of AgentBench</i>!
</p> -->

<p align="center">
ğŸ‘‹ We encourage everyone to explore deeper on VLMs based on our methods!
</p>

## Introduction
Inspired by the dual-stream hypothesis of human vision, which distinguishes the "what" and "where" pathways, we deconstruct the visual processing in VLMs into two parts:
- object recognition -- the "what" way
- spatial perception -- the "where" way

For object recognition, we convert images into *text token maps* and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation.

For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. 

Based on these findings, we introduce *an instruction-agnostic token compression algorithm* based on a plug-and-play visual decoder to improve decoding efficiency, and *a RoPE scaling technique* to enhance spatial reasoning. 
Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.


## Quick Start
### Environment Setup
We recommend you to create a conda environment first as follows:

```shell
conda create -n mm python=3.12
conda activate mm
```

Then clone our repository and install the required packages:

```shell
git clone https://github.com/Siriuslala/vlm_interp.git
cd vlm_interp
pip install -r requirements.txt
```

Before starting, please create a `.env` file, and finish the configuration of the environment variables following the example in `.env_example`:

```shell
ROOT_DIR=./  # where your code is located
DATA_DIR=/path/to/your/data  # where your data is located
WORK_DIR=/path/to/your/work  # intermediate files (e.g., model checkpoints, log files, etc.)
```

### Directory Structure
```
[vlm_interp/]
â”œâ”€â”€ .env_example
â”œâ”€â”€ eval  # evaluation scripts
â”‚   â”œâ”€â”€ CHAIR
â”‚   â”œâ”€â”€ COCO
â”‚   â”œâ”€â”€ GQA
â”‚   â”œâ”€â”€ MMBench
â”‚   â”œâ”€â”€ MME
â”‚   â”œâ”€â”€ POPE
â”‚   â”œâ”€â”€ SQA
â”‚   â”œâ”€â”€ TextVQA
â”‚   â”œâ”€â”€ VQA
â”‚   â”œâ”€â”€ VSR
â”‚   â”œâ”€â”€ WhatsUp
â”‚   â”‚   â”œâ”€â”€ bboxes  # our annotations for WhatsUp
â”‚   â”‚   â””â”€â”€ dataset_zoo
â”‚   â”œâ”€â”€ custom_data.py
â”‚   â””â”€â”€ data_utils.py
â”œâ”€â”€ font
â”‚   â””â”€â”€ SimHei.ttf
â”œâ”€â”€ model  # include the visual decoder and RoPE scaling model
â”‚   â”œâ”€â”€ qwen2_vl_rope_scaling.py
â”‚   â””â”€â”€ unembedding.py
â”œâ”€â”€ patch  # monkey patch files
â”‚   â”œâ”€â”€ intern_2_5_hijack.py
â”‚   â”œâ”€â”€ internvl2_5_utils
â”‚   â”œâ”€â”€ llava1_5_vl_hijack.py
â”‚   â”œâ”€â”€ monkey_patch.py
â”‚   â””â”€â”€ qwen2_5_vl_hijack.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test  # simple tests
â”‚   â”œâ”€â”€ test_intern.py
â”‚   â”œâ”€â”€ test_llava.py
â”‚   â”œâ”€â”€ test_pred.py
â”‚   â”œâ”€â”€ test_pred_batch.py
â”‚   â””â”€â”€ test_qwen.py
â”œâ”€â”€ test_direction.py  # test scripts for the exploration of spatial perception
â”œâ”€â”€ test_pos_embed.py  # test scripts for the exploration of positional embedding
â”œâ”€â”€ test_seg_map.py  # test scripts for the exploration of object recognition
â”œâ”€â”€ train_rope_scaling  # scripts for training the RoPE scaling model
â”‚   â”œâ”€â”€ qwen2_vl_sft_parallel.py
â”‚   â”œâ”€â”€ qwen2_vl_sft_single_gpu.py
â”‚   â”œâ”€â”€ qwen_vl_finetune
â”‚   â””â”€â”€ scripts
â”‚       â”œâ”€â”€ sft.sh
â”‚       â”œâ”€â”€ sft_official.sh
â”‚       â””â”€â”€ sft_parellel.sh
â”œâ”€â”€ train_unembedding  # scripts for training the visual decoder for token compression
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ draw.py
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â””â”€â”€ train.sh
â”‚   â””â”€â”€ train.py
â””â”€â”€ utils
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data_process.py  # scripts for processing the dataset
    â”œâ”€â”€ download.py
    â”œâ”€â”€ draw_file_tree.py
    â”œâ”€â”€ env.sh
    â”œâ”€â”€ font.sh
    â””â”€â”€ intern_vl.py
```

### Object Recognition
The scripts for object recognition are located in `test_seg_map.py`.
This script contains the functions corresponding to the experiments in Section 3 of our paper. Specific usage details can be found in the comments of the script (after `if __name__ == '__main__':`).

Some of the important functions in `test_seg_map.py` are:
- `seg_with_unembedding_tokens`: draw the text maps and segmentation maps for LLaVA;
- `seg_with_unembedding_tokens_qwen`: draw the text maps and segmentation maps for QwenVL;


### Spatial Perception
The scripts for spatial perception are located in `test_direction.py`.
This script contains the functions corresponding to the experiments in Section 4 of our paper. Specific usage details can be found in the comments of the script (after `if __name__ == '__main__':`).

Some of the important functions in `test_direction.py` are:
- `explore_1d_pos_embed_visual_geometry`: plot the 1D positional embedding in visual geometry (Section 4.1);
- `get_relation_representations_layerwise`: plot the relation representations layerwise for left of, right of, behind, in front of (Section 4.2);
- `intervene_in_spatial_reasoning`: intervene in the spatial reasoning process (Section 4.2);
- `erase_object_in_llm`: the `erase` test in Section 4.2;


## Citation

```
@article{li2025reading,
  title={Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models},
  author={Li, Yueyan and Zhao, Chenggong and Zang, Zeyuan and Yuan, Caixia and Wang, Xiaojie},
  journal={arXiv preprint arXiv:2509.19191},
  year={2025}
}
```